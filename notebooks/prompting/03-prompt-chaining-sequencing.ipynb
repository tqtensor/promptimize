{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Chaining and Sequencing Tutorial\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial delves into the powerful techniques of prompt chaining and sequencing when working with large language models. We'll demonstrate these concepts using Amazon Nova via OpenRouter and LangChain, showing you how to create sophisticated, multi-step AI-driven processes.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "As AI applications grow in complexity, it becomes crucial to break down intricate tasks into manageable steps. Prompt chaining and sequencing provide a framework for guiding language models through a series of interconnected prompts, resulting in more structured and controlled outputs. This approach is invaluable for tasks requiring multiple stages of processing or decision-making.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Basic Prompt Chaining**: Linking outputs from one prompt to inputs of another.\n",
        "2. **Sequential Prompting**: Designing a logical progression of prompts to guide the AI through multi-step processes.\n",
        "3. **Dynamic Prompt Generation**: Utilizing the output of one prompt to adaptively create the next prompt.\n",
        "4. **Error Handling and Validation**: Implementing safeguards and quality checks within the prompt chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from os import getenv\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
        "    openai_api_base=getenv(\"OPENROUTER_BASE_URL\"),\n",
        "    model_name=\"bedrock/nova-lite-v1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Prompt Chaining\n",
        "\n",
        "Let's start with a simple example of prompt chaining. We'll create two prompts: one to generate a short story, and another to summarize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Story: The crimson sun dipped below the jagged peaks of Xylos, casting long shadows across the deserted mining colony.  A lone figure, Elara, emerged from the wreckage of her crashed ship, her bio-suit flickering weakly, the only light in the encroaching twilight.  A low growl echoed from the canyons, a sound that promised a far more dangerous threat than the failing oxygen supply.\n",
            "\n",
            "\n",
            "Summary: Stranded and injured after a crash on Xylos, Elara faces dwindling oxygen and a terrifying unknown predator.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates\n",
        "story_prompt = PromptTemplate(\n",
        "    input_variables=[\"genre\"], template=\"Write a short {genre} story in 3-4 sentences.\"\n",
        ")\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"story\"],\n",
        "    template=\"Summarize the following story in one sentence:\\n{story}\",\n",
        ")\n",
        "\n",
        "# Chain the prompts\n",
        "\n",
        "\n",
        "def story_chain(genre):\n",
        "    \"\"\"Generate a story and its summary based on a given genre.\n",
        "\n",
        "    Args:\n",
        "        genre (str): The genre of the story to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the generated story and its summary.\n",
        "    \"\"\"\n",
        "    story = (story_prompt | llm).invoke({\"genre\": genre}).content\n",
        "    summary = (summary_prompt | llm).invoke({\"story\": story}).content\n",
        "    return story, summary\n",
        "\n",
        "\n",
        "# Test the chain\n",
        "genre = \"science fiction\"\n",
        "story, summary = story_chain(genre)\n",
        "print(f\"Story: {story}\\n\\nSummary: {summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential Prompting\n",
        "\n",
        "Now, let's create a more complex sequence of prompts for a multi-step analysis task. We'll analyze a given text for its main theme, tone, and key takeaways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theme: The main theme is the **double-edged sword of artificial intelligence**, encompassing its potential benefits and the crucial need for responsible development to mitigate its risks.\n",
            "\n",
            "\n",
            "Tone: The overall tone of the text is **cautiously optimistic**.  It acknowledges the immense potential of AI (\"revolutionize industries,\" \"improve our daily lives\") while simultaneously highlighting significant risks and the need for careful management (\"ethical questions,\" \"job displacement,\" \"potential for misuse,\" \"caution and foresight\").  The emphasis is on responsible development and mitigating potential harm, making the optimism conditional and tempered.\n",
            "\n",
            "\n",
            "Takeaways: The key takeaways from the text are:\n",
            "\n",
            "* **AI's Dual Nature:** Artificial intelligence presents a double-edged sword, offering immense potential benefits alongside significant risks.  It has the power to revolutionize industries and improve lives, but also poses threats to privacy, employment, and ethical standards.\n",
            "\n",
            "* **Need for Responsible Development:**  The text strongly emphasizes the critical need for careful and responsible development of AI.  This includes proactively addressing ethical concerns and mitigating potential harms before they become widespread problems.\n",
            "\n",
            "* **Cautious Optimism:** While acknowledging the transformative potential of AI, the overall message is one of cautious optimism. The benefits are real and significant, but only if development is guided by ethical considerations and a commitment to minimizing risks.  Unfettered progress is cautioned against.\n",
            "\n",
            "* **Proactive Risk Management:**  The text calls for proactive measures to manage the risks associated with AI, including job displacement and misuse, rather than reacting to problems after they arise.  Foresight and planning are essential.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates for each step\n",
        "theme_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Identify the main theme of the following text:\\n{text}\",\n",
        ")\n",
        "\n",
        "tone_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Describe the overall tone of the following text:\\n{text}\",\n",
        ")\n",
        "\n",
        "takeaway_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"theme\", \"tone\"],\n",
        "    template=\"Given the following text with the main theme '{theme}' and tone '{tone}', what are the key takeaways?\\n{text}\",\n",
        ")\n",
        "\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"Perform a multi-step analysis of a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the theme, tone, and key takeaways of the text.\n",
        "    \"\"\"\n",
        "    theme = (theme_prompt | llm).invoke({\"text\": text}).content\n",
        "    tone = (tone_prompt | llm).invoke({\"text\": text}).content\n",
        "    takeaways = (\n",
        "        (takeaway_prompt | llm)\n",
        "        .invoke({\"text\": text, \"theme\": theme, \"tone\": tone})\n",
        "        .content\n",
        "    )\n",
        "    return {\"theme\": theme, \"tone\": tone, \"takeaways\": takeaways}\n",
        "\n",
        "\n",
        "# Test the sequential prompting\n",
        "sample_text = \"The rapid advancement of artificial intelligence has sparked both excitement and concern among experts. While AI promises to revolutionize industries and improve our daily lives, it also raises ethical questions about privacy, job displacement, and the potential for misuse. As we stand on the brink of this technological revolution, it's crucial that we approach AI development with caution and foresight, ensuring that its benefits are maximized while its risks are minimized.\"\n",
        "\n",
        "analysis = analyze_text(sample_text)\n",
        "for key, value in analysis.items():\n",
        "    print(f\"{key.capitalize()}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Prompt Generation\n",
        "\n",
        "In this section, we'll create a dynamic question-answering system that generates follow-up questions based on previous answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: What are the potential applications of quantum computing?\n",
            "A1: Drug discovery, materials science, financial modeling, cryptography, optimization problems, and artificial intelligence.\n",
            "\n",
            "\n",
            "Q2: What are the biggest challenges currently hindering the widespread adoption and practical application of quantum computing in these fields?\n",
            "\n",
            "A2: High cost, qubit instability (coherence times), limited qubit numbers, and lack of error correction are major hurdles hindering widespread quantum computing adoption.\n",
            "\n",
            "\n",
            "Q3: Given the limitations of high cost, qubit instability, limited qubit numbers, and lack of error correction, what are the most promising research areas currently being pursued to overcome these challenges and accelerate the practical application of quantum computing?\n",
            "\n",
            "A3: Error mitigation techniques, fault-tolerant quantum computing architectures (e.g., topological qubits), novel qubit designs (e.g., trapped ions, neutral atoms), and development of scalable quantum algorithms.\n",
            "\n",
            "\n",
            "Q4: Given the focus on fault-tolerant quantum computing architectures and novel qubit designs, what are the major trade-offs and challenges associated with each approach in terms of scalability, cost, and achievable coherence times?\n",
            "\n",
            "A4: Fault-tolerant architectures trade increased complexity and cost for higher error correction, impacting scalability. Novel qubit designs aim for longer coherence times and reduced cost, but often compromise scalability or require complex control systems.  All approaches face challenges in achieving sufficient coherence for large-scale, fault-tolerant computation.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question concisely:\\n{question}\",\n",
        ")\n",
        "\n",
        "follow_up_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"Based on the question '{question}' and the answer '{answer}', generate a relevant follow-up question.\",\n",
        ")\n",
        "\n",
        "\n",
        "def dynamic_qa(initial_question, num_follow_ups=3):\n",
        "    \"\"\"Conduct a dynamic Q&A session with follow-up questions.\n",
        "\n",
        "    Args:\n",
        "        initial_question (str): The initial question to start the Q&A session.\n",
        "        num_follow_ups (int): The number of follow-up questions to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing questions and answers.\n",
        "    \"\"\"\n",
        "    qa_chain = []\n",
        "    current_question = initial_question\n",
        "\n",
        "    for _ in range(num_follow_ups + 1):  # +1 for the initial question\n",
        "        answer = (answer_prompt | llm).invoke({\"question\": current_question}).content\n",
        "        qa_chain.append({\"question\": current_question, \"answer\": answer})\n",
        "\n",
        "        if _ < num_follow_ups:  # Generate follow-up for all but the last iteration\n",
        "            current_question = (\n",
        "                (follow_up_prompt | llm)\n",
        "                .invoke({\"question\": current_question, \"answer\": answer})\n",
        "                .content\n",
        "            )\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "# Test the dynamic Q&A system\n",
        "initial_question = \"What are the potential applications of quantum computing?\"\n",
        "qa_session = dynamic_qa(initial_question)\n",
        "\n",
        "for i, qa in enumerate(qa_session):\n",
        "    print(f\"Q{i+1}: {qa['question']}\")\n",
        "    print(f\"A{i+1}: {qa['answer']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling and Validation\n",
        "\n",
        "In this final section, we'll implement error handling and validation in our prompt chains to make them more robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final result for topic '\u0110i\u1ec7n Bi\u00ean Ph\u1ee7': 1954\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Generate a 4-digit number related to the topic: {topic}. Respond with ONLY the number, no additional text.\",\n",
        ")\n",
        "\n",
        "validate_prompt = PromptTemplate(\n",
        "    input_variables=[\"number\", \"topic\"],\n",
        "    template=\"Is the number {number} truly related to the topic '{topic}'? Answer with 'Yes' or 'No' and explain why.\",\n",
        ")\n",
        "\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Extract a 4-digit number from the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to extract the number from.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The extracted 4-digit number, or None if no valid number is found.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"\\b\\d{4}\\b\", text)\n",
        "    return match.group() if match else None\n",
        "\n",
        "\n",
        "def robust_number_generation(topic, max_attempts=3):\n",
        "    \"\"\"Generate a topic-related number with validation and error handling.\n",
        "\n",
        "    Args:\n",
        "        topic (str): The topic to generate a number for.\n",
        "        max_attempts (int): Maximum number of generation attempts.\n",
        "\n",
        "    Returns:\n",
        "        str: A validated 4-digit number or an error message.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = (generate_prompt | llm).invoke({\"topic\": topic}).content\n",
        "            number = extract_number(response)\n",
        "\n",
        "            if not number:\n",
        "                raise ValueError(\n",
        "                    f\"Failed to extract a 4-digit number from the response: {response}\"\n",
        "                )\n",
        "\n",
        "            # Validate the relevance\n",
        "            validation = (\n",
        "                (validate_prompt | llm)\n",
        "                .invoke({\"number\": number, \"topic\": topic})\n",
        "                .content\n",
        "            )\n",
        "            if validation.lower().startswith(\"yes\"):\n",
        "                return number\n",
        "            else:\n",
        "                print(\n",
        "                    f\"Attempt {attempt + 1}: Number {number} was not validated. Reason: {validation}\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "\n",
        "    return \"Failed to generate a valid number after multiple attempts.\"\n",
        "\n",
        "\n",
        "# Test the robust number generation\n",
        "topic = \"\u0110i\u1ec7n Bi\u00ean Ph\u1ee7\"\n",
        "result = robust_number_generation(topic)\n",
        "print(f\"Final result for topic '{topic}': {result}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
